<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Dream</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://Zhlblank.github.io/"/>
  <updated>2020-04-21T15:03:43.654Z</updated>
  <id>https://Zhlblank.github.io/</id>
  
  <author>
    <name>ZhaoHongliang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>docker搭建hadoop集群</title>
    <link href="https://Zhlblank.github.io/2020/04/21/docker%E6%90%AD%E5%BB%BAhadoop%E9%9B%86%E7%BE%A4/"/>
    <id>https://Zhlblank.github.io/2020/04/21/docker搭建hadoop集群/</id>
    <published>2020-04-21T07:57:41.000Z</published>
    <updated>2020-04-21T15:03:43.654Z</updated>
    
    <content type="html"><![CDATA[<h5 id="1-Docker安装：（windows下限制较多，docker的linux模式与VM的虚拟服务会冲突，每次使用需要重新开关服务，重启电脑，所以是在windows上的虚拟主机（VM）中实现，此教程适合使用过linux系统的人员）"><a href="#1-Docker安装：（windows下限制较多，docker的linux模式与VM的虚拟服务会冲突，每次使用需要重新开关服务，重启电脑，所以是在windows上的虚拟主机（VM）中实现，此教程适合使用过linux系统的人员）" class="headerlink" title="1.Docker安装：（windows下限制较多，docker的linux模式与VM的虚拟服务会冲突，每次使用需要重新开关服务，重启电脑，所以是在windows上的虚拟主机（VM）中实现，此教程适合使用过linux系统的人员）"></a>1.Docker安装：（windows下限制较多，docker的linux模式与VM的虚拟服务会冲突，每次使用需要重新开关服务，重启电脑，所以是在windows上的虚拟主机（VM）中实现，此教程适合使用过linux系统的人员）</h5><p>docker安装：<a href="https://www.runoob.com/docker/centos-docker-install.html" target="_blank" rel="noopener">https://www.runoob.com/docker/centos-docker-install.html</a></p><p>不同的系统只要安装好docker后，docker的操作都是一样的</p><h5 id="2-docker的基本使用"><a href="#2-docker的基本使用" class="headerlink" title="2.docker的基本使用"></a>2.docker的基本使用</h5><p>启动docker服务</p><p><code>service docker start</code></p><p>docker刚安装好是没有镜像的，我用的ubuntu来搭建hadoop集群，也可以用其他linux发行版</p><p><code>docker pull ubuntu  //拉取也就是在docker中下载ubuntu镜像,默认是最新的</code>等待下载完成</p><p><code>docker images //查看镜像，可以看到刚拉取的ubuntu镜像</code> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@linux:~# docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">ubuntu              latest              72300a873c2c        8 weeks ago         64.2MB</span><br></pre></td></tr></table></figure><p>ok,因为hadoop集群的通信是需要局域网的，所以我们要创建hadoop的专用网络，用桥接模式</p><p><code>docker network create --driver=bridge hadoop</code></p><p>创建好之后查看创建的网络</p><p><code>docker network ls</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">root@liunx:~# docker network ls</span><br><span class="line">NETWORK ID          NAME                DRIVER              SCOPE</span><br><span class="line">a520acd0f5eb        bridge              bridge              local</span><br><span class="line">62cb2d841382        hadoop              bridge              local</span><br><span class="line">b7fef15ea068        host                host                local</span><br><span class="line">ba108fc8779a        none                null                local</span><br></pre></td></tr></table></figure><p>然后用ubuntu镜像运行一个容器</p><p><code>docker run -it ubuntu /bin/bash //i和t分别是以交互模式和终端运行</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@linux:~# docker run -it ubuntu /bin/bash</span><br><span class="line">root@b434aef5bc5f:/#</span><br></pre></td></tr></table></figure><p>exit可以退出容器，ctrl+p+q切换shell到主机而不退出容器</p><p><code>docker ps //显示运行的容器</code></p><p><code>docker start 容器名/容器ID //运行一个存在的容器 stop是停止运行的容器</code></p><p><code>docker attach 容器名/容器ID //进入运行的容器</code>  </p><p>上面的命令之后会经常使用</p><h5 id="3-ubuntu镜像的配置"><a href="#3-ubuntu镜像的配置" class="headerlink" title="3.ubuntu镜像的配置"></a>3.ubuntu镜像的配置</h5><p>做完上面两部分之后，就是配置Ubuntu了，因为初始拉取的ubuntu镜像只有最基础的内核和文件系统，缺少网络工具,jdk,ssh,vim而这些是配置集群不可或缺的部分</p><p>在<code>root@b434aef5bc5f:/#</code> </p><h6 id="1-换源-将下面的内容添加到-etc-apt-sources-list中"><a href="#1-换源-将下面的内容添加到-etc-apt-sources-list中" class="headerlink" title="1.换源,将下面的内容添加到/etc/apt/sources.list中"></a>1.换源,将下面的内容添加到/etc/apt/sources.list中</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial main</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial main</span><br><span class="line"></span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main</span><br><span class="line"></span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial universe</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial universe</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates universe</span><br><span class="line"></span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-security main</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-security universe</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security universe</span><br></pre></td></tr></table></figure><h6 id="2-使用apt-update来更新源"><a href="#2-使用apt-update来更新源" class="headerlink" title="2.使用apt update来更新源"></a>2.使用<code>apt update</code>来更新源</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">apt install net-tools</span><br><span class="line">apt install inetutils-ping//安装ping工具,不能直接apt-get install ping，因为在inetutils里面</span><br><span class="line">apt install openjdk-8-jdk</span><br><span class="line">apt install vim</span><br></pre></td></tr></table></figure><h6 id="3-安装ssh"><a href="#3-安装ssh" class="headerlink" title="3.安装ssh"></a>3.安装ssh</h6><p>ssh是一种应用层的安全远程登录协议，它会生成一对密匙，分别是私匙和公匙，详细解释可以去看ssh官方文档</p><p><code>apt-get install openssh-server</code></p><p><code>apt-get install openssh-client</code></p><p>使用<code>ssh-keygen -t rsa -P &#39;&#39;</code>来创建无密码登录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">root@b434aef5bc5f:/# ssh-keygen -t rsa -P &apos;&apos; </span><br><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Enter file in which to save the key (/root/.ssh/id_rsa): </span><br><span class="line">/root/.ssh/id_rsa already exists.</span><br><span class="line">Overwrite (y/n)? y</span><br><span class="line">Your identification has been saved in /root/.ssh/id_rsa.</span><br><span class="line">Your public key has been saved in /root/.ssh/id_rsa.pub.</span><br><span class="line">The key fingerprint is:</span><br><span class="line">SHA256:p0UBShQiq/AxMBv+HxEku3a6WuwPkVu01E1HNck4kA8 root@cdbc600b933b</span><br><span class="line">The key&apos;s randomart image is:</span><br><span class="line">+---[RSA 2048]----+</span><br><span class="line">|+ ..o++..+=o+o.  |</span><br><span class="line">|.= oooo.oE.+ o.  |</span><br><span class="line">|o.+. +.. .+ .    |</span><br><span class="line">|.o.o= o  . .     |</span><br><span class="line">|. .* =  S o      |</span><br><span class="line">|  o B .  +       |</span><br><span class="line">|   * .  .        |</span><br><span class="line">|  o o            |</span><br><span class="line">| ..o..           |</span><br><span class="line">+----[SHA256]-----+</span><br></pre></td></tr></table></figure><p>cat命令读出id_rsa.pub后以流的方式追加到authorized_keys中</p><p><code>cat .ssh/id_rsa.pub &gt;&gt; .ssh/authorized_keys</code></p><p>启动ssh服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@b434aef5bc5f:~# service ssh start</span><br><span class="line"></span><br><span class="line">Starting OpenBSD Secure Shell server sshd           [ OK ]</span><br></pre></td></tr></table></figure><p>172.17.0.2是容器b434aef5bc5f的IP地址</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">root@b434aef5bc5f:~# ssh 172.17.0.2</span><br><span class="line">Welcome to Ubuntu 18.04.4 LTS (GNU/Linux 5.3.0-kali2-amd64 x86_64)</span><br><span class="line"></span><br><span class="line"> * Documentation:  https://help.ubuntu.com</span><br><span class="line"> * Management:     https://landscape.canonical.com</span><br><span class="line"> * Support:        https://ubuntu.com/advantage</span><br><span class="line">   This system has been minimized by removing packages and content that are</span><br><span class="line">   not required on a system that users do not log into.</span><br><span class="line"></span><br><span class="line">To restore this content, you can run the &apos;unminimize&apos; command.</span><br><span class="line"></span><br><span class="line">The programs included with the Ubuntu system are free software;</span><br><span class="line">the exact distribution terms for each program are described in the</span><br><span class="line">individual files in /usr/share/doc/*/copyright.</span><br><span class="line"></span><br><span class="line">Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by</span><br><span class="line">applicable law.</span><br></pre></td></tr></table></figure><p>出现上述问题ssh -o StrictHostKeyChecking=no <a href="mailto:root@172.17.0.2" target="_blank" rel="noopener">root@172.17.0.2</a></p><p>然后再次执行</p><p>成功！</p><p>方便以后使用，追加<code>service ssh start</code>到<code>.bashrc</code>文件</p><p>vim /root/.bashrc</p><h5 id="4-安装hadoop"><a href="#4-安装hadoop" class="headerlink" title="4.安装hadoop"></a>4.安装hadoop</h5><p>下载</p><p><code>wget http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz</code></p><p>解压</p><p><code>tar -zxvf hadoop-3.2.1.tar.gz -C /usr/local</code></p><p>修改 /etc/profile 文件，添加环境变量</p><p><code>vim /etc/profile</code></p><p>追加以下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">#java</span><br><span class="line">export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</span><br><span class="line">export JRE_HOME=$&#123;JAVA_HOME&#125;/jre    </span><br><span class="line">export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib    </span><br><span class="line">export PATH=$&#123;JAVA_HOME&#125;/bin:$PATH</span><br><span class="line">#hadoop</span><br><span class="line">export HADOOP_HOME=/usr/local/hadoop</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br><span class="line">export HADOOP_COMMON_HOME=$HADOOP_HOME </span><br><span class="line">export HADOOP_HDFS_HOME=$HADOOP_HOME </span><br><span class="line">export HADOOP_MAPRED_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_YARN_HOME=$HADOOP_HOME </span><br><span class="line">export HADOOP_INSTALL=$HADOOP_HOME </span><br><span class="line">export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native </span><br><span class="line">export HADOOP_CONF_DIR=$HADOOP_HOME </span><br><span class="line">export HADOOP_LIBEXEC_DIR=$HADOOP_HOME/libexec </span><br><span class="line">export JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native:$JAVA_LIBRARY_PATH</span><br><span class="line">export HADOOP_CONF_DIR=$HADOOP_PREFIX/etc/hadoop</span><br><span class="line">export HDFS_DATANODE_USER=root</span><br><span class="line">export HDFS_DATANODE_SECURE_USER=root</span><br><span class="line">export HDFS_SECONDARYNAMENODE_USER=root</span><br><span class="line">export HDFS_NAMENODE_USER=root</span><br><span class="line">export YARN_RESOURCEMANAGER_USER=root</span><br><span class="line">export YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure><p>export HADOOP_CONF_DIR=$HADOOP_PREFIX/etc/hadoop 需要改为</p><p>export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop否则运行hadoop命令需要找到路径并加./</p><p>如果不嫌繁琐的话也可以不用改，最后几行代表hadoop是哪个用户，我用的root</p><p>运行<code>source /etc/profile</code>使其生效</p><p>在目录 <code>/usr/local/hadoop/etc/hadoop</code> 下</p><p> hadoop-env.sh 文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</span><br><span class="line">export HDFS_NAMENODE_USER=root</span><br><span class="line">export HDFS_DATANODE_USER=root</span><br><span class="line">export HDFS_SECONDARYNAMENODE_USER=root</span><br><span class="line">export YARN_RESOURCEMANAGER_USER=root</span><br><span class="line">export YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure><p> core-site.xml 文件 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.default.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://h01:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/home/hadoop3/hadoop/tmp&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p> hdfs-site.xml文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;2&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line"> &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/home/hadoop3/hadoop/hdfs/name&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.data.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/home/hadoop3/hadoop/hdfs/data&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>mapred-site.xml文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.application.classpath&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;</span><br><span class="line">            /usr/local/hadoop/etc/hadoop,</span><br><span class="line">            /usr/local/hadoop/share/hadoop/common/*,</span><br><span class="line">            /usr/local/hadoop/share/hadoop/common/lib/*,</span><br><span class="line">            /usr/local/hadoop/share/hadoop/hdfs/*,</span><br><span class="line">            /usr/local/hadoop/share/hadoop/hdfs/lib/*,</span><br><span class="line">            /usr/local/hadoop/share/hadoop/mapreduce/*,</span><br><span class="line">            /usr/local/hadoop/share/hadoop/mapreduce/lib/*,</span><br><span class="line">            /usr/local/hadoop/share/hadoop/yarn/*,</span><br><span class="line">            /usr/local/hadoop/share/hadoop/yarn/lib/*</span><br><span class="line">        &lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>yarn-site.xml文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;h01&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p> worker文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">h00</span><br><span class="line">h01</span><br><span class="line">h02</span><br></pre></td></tr></table></figure><p>hadoop终于搞完了</p><h5 id="5-搭建集群"><a href="#5-搭建集群" class="headerlink" title="5.搭建集群"></a>5.搭建集群</h5><p>上述步骤相当于在实体机搭建了一台分布式主机，但是我们需要的是hadoop集群，怎么能一台机器呢</p><p>所以我们要横向扩展，这也是hadoop集群的特性，易于扩展，也就是死命加机器(手动滑稽)。</p><p>在docker中就很方便了，将搞完hadoop的这个容器，也就相当于现实中，一台运行着的机器，直接将其复制无数相同的<strong>从节点机</strong>，当然我们只要两个从节点就够了，也就是第四步中worker文件的配置，h00为主节点，h01,h02为从节点。好了闲话不多说实操起来。</p><p>将容器打包成镜像，因为我们要扩展呀，必须得有镜像</p><p><code>docker commit -m &quot;haddop&quot; -a &quot;hadoop&quot; b434aef5bc5f  myhadoop</code></p><p><code>docker images</code>查看，里面出现了myhadoop</p><h6 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h6><p>首先就是主节点，虽然之前的也可以用，但是我们要配置端口映射，不然无法在docker之外操作hadoop集群</p><p><code>docker run -it --network hadoop -h h01 --name &quot;h00&quot; -p 9870:9870 -p 8088:8088 -p 9000:9000 myhadoop /bin/bash   //前两个端口是web端，9000是hdfs文件系统的端口映射</code></p><p>好了主节点配置好了，我们的hadoop集群有了大脑，然后增加两个从节点</p><p><code>docker run -it --network hadoop -h h01 --name &quot;h02&quot; myhadoop /bin/bash</code></p><p><code>docker run -it --network hadoop -h h02 --name &quot;h02&quot; myhadoop /bin/bash</code>    </p><p>在客户机中创建docker.sh启动脚本里面添加下面内容，并且用<code>chmod +x hadoop.sh</code>使其可执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">service docker start</span><br><span class="line">docker start h01</span><br><span class="line">docker start h02</span><br><span class="line">docker start h03</span><br><span class="line">docker attach h01</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root@linux:~# ./docker.sh </span><br><span class="line">h01</span><br><span class="line">h02</span><br><span class="line">h03</span><br><span class="line">root@h01:/# cat hadoop.sh </span><br><span class="line">./usr/local/hadoop/sbin/start-all.sh</span><br><span class="line">root@h01:/#</span><br></pre></td></tr></table></figure><h6 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h6><p>在h01节点中先初始化hdfs，以后启动不用初始化，否则会报错</p><p><code>root@h01:/usr/local/hadoop/bin#./hadoop namenode -format</code></p><p>在<code>/</code>目录下创建hadoop集群启动脚本hadoop.sh，写入下面内容</p><p><code>./usr/local/hadoop/sbin/start-all.sh</code></p><p>同docker启动脚本一样</p><p>每次启动h01运行source /etc/profile,执行命令就不用加./</p><p>好了，hadoop完全分布式集群搭建到此结束</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h5 id=&quot;1-Docker安装：（windows下限制较多，docker的linux模式与VM的虚拟服务会冲突，每次使用需要重新开关服务，重启电脑，所以是在windows上的虚拟主机（VM）中实现，此教程适合使用过linux系统的人员）&quot;&gt;&lt;a href=&quot;#1-Docke
      
    
    </summary>
    
      <category term="docker" scheme="https://Zhlblank.github.io/categories/docker/"/>
    
    
      <category term="hadoop" scheme="https://Zhlblank.github.io/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://Zhlblank.github.io/2019/04/07/hello-world/"/>
    <id>https://Zhlblank.github.io/2019/04/07/hello-world/</id>
    <published>2019-04-07T15:36:35.730Z</published>
    <updated>2019-04-07T15:36:35.730Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
